{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Imports**\n",
        "\n"
      ],
      "metadata": {
        "id": "swRKrrNrGye8"
      },
      "id": "swRKrrNrGye8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6e944705",
      "metadata": {
        "id": "6e944705"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import silhouette_score\n",
        "from geopy.distance import geodesic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression function**"
      ],
      "metadata": {
        "id": "1cVyCp9N3u9b"
      },
      "id": "1cVyCp9N3u9b"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c880931a",
      "metadata": {
        "id": "c880931a"
      },
      "outputs": [],
      "source": [
        "class LinearRegression1:\n",
        "    def __init__(self):\n",
        "        self.coefficients = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Add a column of ones to X for the intercept term\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        # Calculate the coefficients using the pseudoinverse\n",
        "        self.coefficients = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add a column of ones to X for the intercept term\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "\n",
        "        # Calculate the predicted values\n",
        "        return X.dot(self.coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression function**"
      ],
      "metadata": {
        "id": "Idsy7fec4IoL"
      },
      "id": "Idsy7fec4IoL"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2cf9ed4b",
      "metadata": {
        "id": "2cf9ed4b"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression1:\n",
        "    '''\n",
        "    A class which implements logistic regression model with gradient descent.\n",
        "    '''\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=3000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights, self.bias = None, None\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(x):\n",
        "        '''\n",
        "        Private method, used to pass results of the line equation through the sigmoid function.\n",
        "\n",
        "        :param x: float, prediction made by the line equation\n",
        "        :return: float\n",
        "        '''\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Used to calculate the coefficient of the logistic regression model.\n",
        "\n",
        "        :param X: array, features\n",
        "        :param y: array, true values\n",
        "        :return: None\n",
        "        '''\n",
        "\n",
        "        # 1. Initialize coefficients\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "        self.bias = 0\n",
        "\n",
        "        # 2. Perform gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            linear_pred = np.dot(X, self.weights) + self.bias\n",
        "            probability = self._sigmoid(linear_pred)\n",
        "\n",
        "            # Calculate derivatives\n",
        "            partial_w = (1 / X.shape[0]) * (np.dot(X.T, (probability - y)))\n",
        "            partial_d = (1 / X.shape[0]) * (np.sum(probability - y))\n",
        "\n",
        "            # Update the coefficients\n",
        "            self.weights -= self.learning_rate * partial_w\n",
        "            self.bias -= self.learning_rate * partial_d\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        '''\n",
        "        Calculates prediction probabilities for a given threshold using the line equation\n",
        "        passed through the sigmoid function.\n",
        "\n",
        "        :param X: array, features\n",
        "        :return: array, prediction probabilities\n",
        "        '''\n",
        "        linear_pred = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_pred)\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        '''\n",
        "        Makes predictions using the line equation passed through the sigmoid function.\n",
        "\n",
        "        :param X: array, features\n",
        "        :param threshold: float, classification threshold\n",
        "        :return: array, predictions\n",
        "        '''\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return [1 if i > threshold else 0 for i in probabilities]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA function**"
      ],
      "metadata": {
        "id": "Hm5WOcSl4VQ4"
      },
      "id": "Hm5WOcSl4VQ4"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d33c1e45",
      "metadata": {
        "id": "d33c1e45"
      },
      "outputs": [],
      "source": [
        "class PCA1:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "        self.mean = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Compute the mean of the data\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "\n",
        "        # Center the data\n",
        "\n",
        "        # Compute the covariance matrix\n",
        "        cov_matrix = np.cov(X,rowvar=0, bias=1)\n",
        "\n",
        "        # Perform eigenvalue decomposition\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "        # Sort the eigenvalues and corresponding eigenvectors in descending order\n",
        "        idx = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[idx]\n",
        "        eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "        # Select the top n_components eigenvectors\n",
        "        self.components = eigenvectors.T[:self.n_components]\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Center the data\n",
        "        return np.dot(X, self.components.T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **features engineering**"
      ],
      "metadata": {
        "id": "SMxejDMQ6TvW"
      },
      "id": "SMxejDMQ6TvW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Attempting to identify and select specific amenities that might be indicative of whether a property is expensive"
      ],
      "metadata": {
        "id": "jeGGSqy8Atix"
      },
      "id": "jeGGSqy8Atix"
    },
    {
      "cell_type": "code",
      "source": [
        "def check_amenities(train):\n",
        "\n",
        "    # Clean amenities strings by removing square brackets if present\n",
        "    cleaned_strings = [\n",
        "        amenities_string[2:-2] if amenities_string.startswith('[\"') and amenities_string.endswith('\"]') else amenities_string\n",
        "        for amenities_string in train['amenities']\n",
        "    ]\n",
        "\n",
        "    # Extract words from each cleaned string\n",
        "    words_lists = [\n",
        "        [word.strip('\" ,') for word in cleaned_string.split(',')]\n",
        "        for cleaned_string in cleaned_strings\n",
        "    ]\n",
        "\n",
        "    # One-hot encode amenities using pandas get_dummies\n",
        "    amenities_df = train['amenities'].str.get_dummies(sep=', ')\n",
        "\n",
        "    # Calculate the count of each amenity\n",
        "    amenity_counts = amenities_df.apply(pd.Series.value_counts).sum().sort_values(ascending=False)\n",
        "\n",
        "    # Combine the original DataFrame with the one-hot encoded amenities\n",
        "    combined_df = pd.concat([train, amenities_df], axis=1)\n",
        "\n",
        "    # Calculate the correlation between amenities and the target variable 'expensive'\n",
        "    correlation_matrix = combined_df[amenities_df.columns].corrwith(combined_df['expensive'])\n",
        "\n",
        "    # Sort the correlations in descending order\n",
        "    sorted_correlations = correlation_matrix.sort_values(ascending=False)\n",
        "\n",
        "    # Set a threshold for selecting features based on correlation\n",
        "    threshold = 0.2\n",
        "\n",
        "    # Select features (amenities) with correlation above the threshold\n",
        "    selected_feature_names = sorted_correlations[sorted_correlations > threshold].index.tolist()\n",
        "\n",
        "    # Remove double quotes from selected feature names\n",
        "    selected_feature_names = [word.replace('\"', '') for word in selected_feature_names]\n",
        "\n",
        "    # Check if each property has at least one of the selected amenities\n",
        "    is_in_selected_amenities = []\n",
        "\n",
        "    for amenity_list in words_lists:\n",
        "        flag = 0\n",
        "        for good_word in amenity_list:\n",
        "            if good_word in selected_feature_names:\n",
        "                flag = 1\n",
        "                break\n",
        "        is_in_selected_amenities.append(flag)\n",
        "\n",
        "    # Add a new column indicating whether selected amenities are present\n",
        "    train['Selected_Feature_Names'] = is_in_selected_amenities\n",
        "\n",
        "    # Return the updated DataFrame and the list of selected feature names\n",
        "    return train, selected_feature_names\n"
      ],
      "metadata": {
        "id": "2fYulFGbUk1z"
      },
      "id": "2fYulFGbUk1z",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "--- Same function for test data\n"
      ],
      "metadata": {
        "id": "vCUX-Ah8Bm5d"
      },
      "id": "vCUX-Ah8Bm5d"
    },
    {
      "cell_type": "code",
      "source": [
        "def check_amenities_test(test, train):\n",
        "    # Clean amenities strings by removing square brackets if present\n",
        "    cleaned_strings = [\n",
        "        amenities_string[2:-2] if amenities_string.startswith('[\"') and amenities_string.endswith('\"]') else amenities_string\n",
        "        for amenities_string in test['amenities']\n",
        "    ]\n",
        "\n",
        "    # Extract words from each cleaned string\n",
        "    words_lists = [\n",
        "        [word.strip('\" ,') for word in cleaned_string.split(',')]\n",
        "        for cleaned_string in cleaned_strings\n",
        "    ]\n",
        "\n",
        "    # Initialize a list to store whether selected amenities are present\n",
        "    is_in_selected_amenities = []\n",
        "\n",
        "    # Get the selected feature names from the training set\n",
        "    _, selected_feature_names = check_amenities(train)\n",
        "\n",
        "    # Check if each property in the test set has at least one of the selected amenities\n",
        "    for amenity_list in words_lists:\n",
        "        flag = 0\n",
        "        for good_word in amenity_list:\n",
        "            if good_word in selected_feature_names:\n",
        "                flag = 1\n",
        "                break\n",
        "        is_in_selected_amenities.append(flag)\n",
        "\n",
        "    # Add a new column indicating whether selected amenities are present in the test set\n",
        "    test['Selected_Feature_Names'] = is_in_selected_amenities\n",
        "\n",
        "    return test\n"
      ],
      "metadata": {
        "id": "e21OSYjWmCfB"
      },
      "id": "e21OSYjWmCfB",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Data preprocessing - cleaning and transforming various columns in the dataset to make them suitable for analysis or modeling. The preprocessing includes handling percentages, mapping binary categories, converting date-related columns, normalizing availability values, aggregating review scores, and calculating the days since the last review.\n",
        "\n"
      ],
      "metadata": {
        "id": "nkupOj5MB04i"
      },
      "id": "nkupOj5MB04i"
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(train):\n",
        "    # Remove '%' from 'host_response_rate' and 'host_acceptance_rate'\n",
        "    train['host_response_rate'] = train['host_response_rate'].str.replace('%', '')\n",
        "    train['host_acceptance_rate'] = train['host_acceptance_rate'].str.replace('%', '')\n",
        "\n",
        "    # Convert 'host_response_rate' and 'host_acceptance_rate' columns to numeric\n",
        "    train['host_response_rate'] = pd.to_numeric(train['host_response_rate'])\n",
        "    train['host_acceptance_rate'] = pd.to_numeric(train['host_acceptance_rate'])\n",
        "\n",
        "    # Map binary categorical columns 'f' to 0 and 't' to 1\n",
        "    train['instant_bookable'] = train['instant_bookable'].map({'f': 0, 't': 1})\n",
        "    train['host_is_superhost'] = train['host_is_superhost'].map({'f': 0, 't': 1})\n",
        "\n",
        "    # Convert 'host_since' to datetime format and calculate months since host registration\n",
        "    train['host_since'] = pd.to_datetime(train['host_since'])\n",
        "    current_date = pd.to_datetime(datetime.now().date())\n",
        "    train['host_since'] = (current_date - train['host_since']) / np.timedelta64(1, 'M')\n",
        "\n",
        "    # Normalize availability related columns\n",
        "    train[\"availability_60\"] = train[\"availability_60\"] / 2\n",
        "    train[\"availability_90\"] = train[\"availability_90\"] / 3\n",
        "    train[\"availability_365\"] = train[\"availability_365\"] / 12\n",
        "    train[\"availability\"] = (train[\"availability_365\"] + train[\"availability_90\"] + train[\"availability_60\"] + train[\"availability_30\"]) / 30\n",
        "\n",
        "    # Calculate an overall review score by averaging individual scores\n",
        "    train[\"review_scores\"] = (train[\"review_scores_rating\"] + train[\"review_scores_accuracy\"] + train[\"review_scores_cleanliness\"] +\n",
        "                              train[\"review_scores_checkin\"] + train[\"review_scores_communication\"] + train[\"review_scores_location\"] +\n",
        "                              train[\"review_scores_value\"]) / 7\n",
        "\n",
        "    # Convert 'last_review' to datetime format and calculate days since last review\n",
        "    train['last_review'] = pd.to_datetime(train['last_review'])\n",
        "    current_date = pd.to_datetime(datetime.now().date())\n",
        "    train['days_since_last_review'] = (current_date - train['last_review']).dt.days\n",
        "\n",
        "    return train\n"
      ],
      "metadata": {
        "id": "n455szxOBHrj"
      },
      "id": "n455szxOBHrj",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">  I checked the correlation of the columns with the target variable and I remove certain columns from the DataFrame, because they are not needed for the analysis or modeling phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "HMtvKeUgCZL4"
      },
      "id": "HMtvKeUgCZL4"
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns(train):\n",
        "    # Define a list of columns to be dropped\n",
        "    columns_to_drop = [\n",
        "        'id', 'host_id', 'host_listings_count',\n",
        "        'host_has_profile_pic', 'host_identity_verified', 'latitude', 'longitude', 'amenities', 'has_availability',\n",
        "        'availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews',\n",
        "        'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
        "        'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value',\n",
        "        'host_verifications', 'license', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
        "        'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights',\n",
        "        'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calculated_host_listings_count_shared_rooms',\n",
        "        'calculated_host_listings_count_private_rooms'\n",
        "    ]\n",
        "\n",
        "    # Drop the specified columns from the DataFrame\n",
        "    train = train.drop(columns_to_drop, axis=1)\n",
        "\n",
        "    # Return the modified DataFrame\n",
        "    return train\n"
      ],
      "metadata": {
        "id": "x1MGEQ4TSGQm"
      },
      "id": "x1MGEQ4TSGQm",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Prepare the data for machine learning models by converting categorical variables into a format that can be used in numerical calculations by One-hot encoding.\n"
      ],
      "metadata": {
        "id": "9kgmjtZ0DMu2"
      },
      "id": "9kgmjtZ0DMu2"
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(train):\n",
        "    # Identify categorical columns in the DataFrame\n",
        "    categorical_columns = train.select_dtypes(include='object').columns\n",
        "\n",
        "    # Apply one-hot encoding to all categorical columns\n",
        "    df_encoded = pd.get_dummies(train[categorical_columns], drop_first=True)\n",
        "\n",
        "    # Drop the original categorical columns from the original DataFrame\n",
        "    train = train.drop(categorical_columns, axis=1)\n",
        "\n",
        "    # Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
        "    combined_df = pd.concat([train, df_encoded], axis=1)\n",
        "\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "J5Dw_VzkMrGJ"
      },
      "id": "J5Dw_VzkMrGJ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Normalize the scale of numeric features.\n",
        "\n"
      ],
      "metadata": {
        "id": "ms7bszedDiXx"
      },
      "id": "ms7bszedDiXx"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalization(train):\n",
        "\n",
        "  numeric_columns = train.select_dtypes(include='number').columns\n",
        "\n",
        "  # Create a MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  # Apply Min-Max scaling to the selected numeric columns\n",
        "  train[numeric_columns] = scaler.fit_transform(train[numeric_columns])\n",
        "  return train\n"
      ],
      "metadata": {
        "id": "mAk-Cr5dPnwT"
      },
      "id": "mAk-Cr5dPnwT",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Standardize the scale of numeric features.\n",
        "\n"
      ],
      "metadata": {
        "id": "pIlUAvOxDq5e"
      },
      "id": "pIlUAvOxDq5e"
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization(train):\n",
        "\n",
        "  numeric_columns = train.select_dtypes(include='number').columns.difference(['expensive'])\n",
        "  # Create a StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  # Apply standardization to the selected numeric columns\n",
        "  train[numeric_columns] = scaler.fit_transform(train[numeric_columns])\n",
        "  return train\n"
      ],
      "metadata": {
        "id": "yJAwRyvAP2ev"
      },
      "id": "yJAwRyvAP2ev",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "--- Standardize the scale of numeric features for the test (without target column).\n",
        "\n"
      ],
      "metadata": {
        "id": "L674S2zEDw_e"
      },
      "id": "L674S2zEDw_e"
    },
    {
      "cell_type": "code",
      "source": [
        "def standardization_test(train):\n",
        "\n",
        "  numeric_columns = train.select_dtypes(include='number').columns\n",
        "  # Create a StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  # Apply standardization to the selected numeric columns\n",
        "  train[numeric_columns] = scaler.fit_transform(train[numeric_columns])\n",
        "  return train"
      ],
      "metadata": {
        "id": "hGLLpfsSIPg_"
      },
      "id": "hGLLpfsSIPg_",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Handle missing values - fill missing values by Multivariate_impute/KNN - (no NaN value in categorical columns).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXAO2HMIE6H-"
      },
      "id": "bXAO2HMIE6H-"
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_NA(train):\n",
        "  numerical_cols = train.select_dtypes(include='number').columns\n",
        "\n",
        "  # # Impute missing values using KNN\n",
        "  # imputer = KNNImputer(n_neighbors=8, weights=\"uniform\")\n",
        "  # train[numerical_cols] = pd.DataFrame(imputer.fit_transform(train[numerical_cols]), columns=numerical_cols)\n",
        "\n",
        "  # Impute missing values using IterativeImputer\n",
        "  imputer_numeric = IterativeImputer(random_state=0)\n",
        "  train[numerical_cols] = pd.DataFrame(imputer_numeric.fit_transform(train[numerical_cols]), columns=numerical_cols)\n",
        "  return train\n",
        "\n"
      ],
      "metadata": {
        "id": "xwimf-IaQNOo"
      },
      "id": "xwimf-IaQNOo",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> balances the dataset using the Synthetic Minority Over-sampling Technique (SMOTE) - SMOTE is being applied to balance the distribution of the 'expensive' class.\n",
        "\n"
      ],
      "metadata": {
        "id": "cnRrAGglMlFV"
      },
      "id": "cnRrAGglMlFV"
    },
    {
      "cell_type": "code",
      "source": [
        "def balance_SMOTE(train):\n",
        "    # Separate features (X) and target variable (y)\n",
        "    X = train.drop('expensive', axis=1)  # Features\n",
        "    y = train['expensive']  # Target\n",
        "\n",
        "    # Create an instance of SMOTE\n",
        "    smote = SMOTE()\n",
        "\n",
        "    # Apply SMOTE to the dataset\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    # Create a new DataFrame with resampled data\n",
        "    train = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['expensive'])], axis=1)\n",
        "\n",
        "    return train\n"
      ],
      "metadata": {
        "id": "4nJJ5_VcQb-j"
      },
      "id": "4nJJ5_VcQb-j",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The purpose of this function appears to be to replace the original 'bathrooms_text' and 'property_type' columns with their respective percentage columns, representing the percentage of the 'expensive' class within each category.\n",
        "\n"
      ],
      "metadata": {
        "id": "kpn-4HDYOt7B"
      },
      "id": "kpn-4HDYOt7B"
    },
    {
      "cell_type": "code",
      "source": [
        "def change_categorial_cloumns_train(train):\n",
        "    # Step 1: Calculate the sum of expensive values for each unique category\n",
        "    sum_expensive = train.groupby(\"bathrooms_text\")[\"expensive\"].sum()\n",
        "\n",
        "    # Step 2: Calculate the total count of each unique category\n",
        "    total_count = train.groupby(\"bathrooms_text\").size()\n",
        "\n",
        "    # Step 3: Calculate the ratio of sum_expensive to total_count for each category\n",
        "    expensive_ratio = sum_expensive / total_count\n",
        "\n",
        "    # Step 4: Merge the calculated ratio back into the original DataFrame\n",
        "    train = pd.merge(train, expensive_ratio.rename('percentage_bathrooms_text'), left_on='bathrooms_text', right_index=True, how='left')\n",
        "\n",
        "    train = train.drop(['bathrooms_text', 'property_type'], axis=1)\n",
        "\n",
        "    return train"
      ],
      "metadata": {
        "id": "HtE81HJIuTGU"
      },
      "id": "HtE81HJIuTGU",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "--- Same function for test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "bIqA2POCQvXR"
      },
      "id": "bIqA2POCQvXR"
    },
    {
      "cell_type": "code",
      "source": [
        "def change_categorial_columns_test(test, train):\n",
        "\n",
        "    # Step 1: Calculate the sum of expensive values for each unique category\n",
        "    sum_expensive = train.groupby(\"bathrooms_text\")[\"expensive\"].sum()\n",
        "\n",
        "    # Step 2: Calculate the total count of each unique category\n",
        "    total_count = train.groupby(\"bathrooms_text\").size()\n",
        "\n",
        "    # Step 3: Calculate the ratio of sum_expensive to total_count for each category\n",
        "    expensive_ratio = sum_expensive / total_count\n",
        "\n",
        "    # Step 4: Merge the calculated ratio back into the original DataFrame\n",
        "    train = pd.merge(train, expensive_ratio.rename('percentage_bathrooms_text'), left_on='bathrooms_text', right_index=True, how='left')\n",
        "\n",
        "\n",
        "    # Step 5: Merge the calculated ratio from the training set into the test set\n",
        "    test = pd.merge(test, expensive_ratio.rename('percentage_bathrooms_text'), left_on='bathrooms_text', right_index=True, how='left')\n",
        "\n",
        "    # Step 6: Fill missing values with a default value (e.g., 0) in the test set\n",
        "    mean_percentage_train = expensive_ratio.mean()\n",
        "    test['percentage_bathrooms_text'].fillna(mean_percentage_train, inplace=True)\n",
        "\n",
        "    # Optional: Drop unnecessary columns if needed\n",
        "    test = test.drop(['bathrooms_text', 'property_type'], axis=1)\n",
        "\n",
        "    return test\n"
      ],
      "metadata": {
        "id": "X1kysM8BuWlU"
      },
      "id": "X1kysM8BuWlU",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> calculate the centroid coordinates using the KMeans clustering algorithm for both expensive and non-expensive properties. Determine K=4 by elbow method and silhouette score methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zln1pF-DRWdl"
      },
      "id": "Zln1pF-DRWdl"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_centroids(train):\n",
        "    # Extract expensive and non-expensive rows\n",
        "    expensive_rows = train[train['expensive'] == 1][['latitude', 'longitude']]\n",
        "    expensive_rows_0 = train[train['expensive'] == 0][['latitude', 'longitude']]\n",
        "\n",
        "    # Fit KMeans with 4 clusters for expensive properties\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42).fit(expensive_rows)\n",
        "    centroid_coordinates = kmeans.cluster_centers_\n",
        "\n",
        "    # Fit KMeans with 4 clusters for non-expensive properties\n",
        "    kmeans_0 = KMeans(n_clusters=4, random_state=42).fit(expensive_rows_0)\n",
        "    centroid_coordinates_0 = kmeans_0.cluster_centers_\n",
        "\n",
        "    return centroid_coordinates,centroid_coordinates_0"
      ],
      "metadata": {
        "id": "aADcpU-feuGH"
      },
      "id": "aADcpU-feuGH",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Calculates the minimum geodesic distance from each property to the centroids of expensive properties.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vMVd8U45UDop"
      },
      "id": "vMVd8U45UDop"
    },
    {
      "cell_type": "code",
      "source": [
        "def new_centers_train(train,centroid_coordinates):\n",
        "\n",
        "  def calculate_min_distance(row):\n",
        "      property_coordinates = (row['latitude'], row['longitude'])\n",
        "      min_distance = min([geodesic(property_coordinates, location).km for location in centroid_coordinates])\n",
        "      return min_distance\n",
        "  # Create a new column for minimum distance in the DataFrame\n",
        "  train['min_distance_to_top'] = train.apply(calculate_min_distance, axis=1)\n",
        "  return train"
      ],
      "metadata": {
        "id": "Q6k1j5_dXiX3"
      },
      "id": "Q6k1j5_dXiX3",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---Same function for the test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "U765-kqkUK4T"
      },
      "id": "U765-kqkUK4T"
    },
    {
      "cell_type": "code",
      "source": [
        "def new_centers_test(test,centroid_coordinates):\n",
        "\n",
        "  def calculate_min_distance(row):\n",
        "      property_coordinates = (row['latitude'], row['longitude'])\n",
        "      min_distance = min([geodesic(property_coordinates, location).km for location in centroid_coordinates])\n",
        "      return min_distance\n",
        "  # Create a new column for minimum distance in the DataFrame\n",
        "  test['min_distance_to_top'] = test.apply(calculate_min_distance, axis=1)\n",
        "  return test"
      ],
      "metadata": {
        "id": "7Kb4itvAZF8N"
      },
      "id": "7Kb4itvAZF8N",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Calculates the minimum geodesic distances from each property to both expensive and non-expensive centroids and assigns a binary value (1 or 0) based on which set of centroids is closer.\n",
        "\n"
      ],
      "metadata": {
        "id": "ifqeJNk3UlBw"
      },
      "id": "ifqeJNk3UlBw"
    },
    {
      "cell_type": "code",
      "source": [
        "def new_centers_train_binary(train,centroid_coordinates,centroid_coordinates_0):\n",
        "\n",
        "    def calculate_min_distance(row):\n",
        "        property_coordinates = (row['latitude'], row['longitude'])\n",
        "\n",
        "        # Calculate minimum distance to expensive centroids\n",
        "        min_distance_expensive = min([geodesic(property_coordinates, location).km for location in centroid_coordinates])\n",
        "\n",
        "        # Calculate minimum distance to non-expensive centroids\n",
        "        min_distance_non_expensive = min([geodesic(property_coordinates, location).km for location in centroid_coordinates_0])\n",
        "\n",
        "        # Set the new column value based on the minimum distance\n",
        "        if min_distance_expensive <= min_distance_non_expensive:\n",
        "            return 1  # Minimum distance to expensive centroid\n",
        "        else:\n",
        "            return 0  # Minimum distance to non-expensive centroid\n",
        "\n",
        "    # Create a new column for minimum distance classification in your DataFrame\n",
        "    train['min_distance_to_top_binary'] = train.apply(calculate_min_distance, axis=1)\n",
        "\n",
        "    return train"
      ],
      "metadata": {
        "id": "RRr9WuzxXnE1"
      },
      "id": "RRr9WuzxXnE1",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---Same function for the test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "rgDV7cAdUqFB"
      },
      "id": "rgDV7cAdUqFB"
    },
    {
      "cell_type": "code",
      "source": [
        "def new_centers_test_binary(test,centroid_coordinates,centroid_coordinates_0):\n",
        "\n",
        "    def calculate_min_distance(row):\n",
        "        property_coordinates = (row['latitude'], row['longitude'])\n",
        "\n",
        "        # Calculate minimum distance to expensive centroids\n",
        "        min_distance_expensive = min([geodesic(property_coordinates, location).km for location in centroid_coordinates])\n",
        "\n",
        "        # Calculate minimum distance to non-expensive centroids\n",
        "        min_distance_non_expensive = min([geodesic(property_coordinates, location).km for location in centroid_coordinates_0])\n",
        "\n",
        "        # Set the new column value based on the minimum distance\n",
        "        if min_distance_expensive <= min_distance_non_expensive:\n",
        "            return 1  # Minimum distance to expensive centroid\n",
        "        else:\n",
        "            return 0  # Minimum distance to non-expensive centroid\n",
        "\n",
        "    # Create a new column for minimum distance classification in your DataFrame\n",
        "    test['min_distance_to_top_binary'] = test.apply(calculate_min_distance, axis=1)\n",
        "\n",
        "    return test\n"
      ],
      "metadata": {
        "id": "mhVjYQDFamgs"
      },
      "id": "mhVjYQDFamgs",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Recipe**"
      ],
      "metadata": {
        "id": "gtd0YMkbY57t"
      },
      "id": "gtd0YMkbY57t"
    },
    {
      "cell_type": "code",
      "source": [
        "def data_recipe(train):\n",
        "  centroid_coordinates , centroid_coordinates_0 = calculate_centroids(train)\n",
        "  train = new_centers_train(train,centroid_coordinates)\n",
        "  train = new_centers_train_binary(train,centroid_coordinates,centroid_coordinates_0)\n",
        "  train = change_categorial_cloumns_train(train)\n",
        "  train, _ = check_amenities(train)\n",
        "  train = data_preprocessing(train)\n",
        "  train = drop_columns(train)\n",
        "  train = fill_NA(train)\n",
        "  train = encoding(train)\n",
        "  train = balance_SMOTE(train)\n",
        "  # train = normalization(train)\n",
        "  train = standardization(train)\n",
        "  return train\n"
      ],
      "metadata": {
        "id": "_FCX7z0exmXL"
      },
      "id": "_FCX7z0exmXL",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_recipe_test(test,train):\n",
        "  centroid_coordinates , centroid_coordinates_0 = calculate_centroids(train)\n",
        "  test = new_centers_test(test,centroid_coordinates)\n",
        "  test = new_centers_test_binary(test,centroid_coordinates,centroid_coordinates_0)\n",
        "  test = change_categorial_columns_test(test,train)\n",
        "  test = check_amenities_test(test,train)\n",
        "  test = data_preprocessing(test)\n",
        "  test = drop_columns(test)\n",
        "  test = fill_NA(test)\n",
        "  test = encoding(test)\n",
        "  # test = normalization(test)\n",
        "  test = standardization(test)\n",
        "  return test"
      ],
      "metadata": {
        "id": "8vhp724nXere"
      },
      "id": "8vhp724nXere",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train model**"
      ],
      "metadata": {
        "id": "_dw60haPZARb"
      },
      "id": "_dw60haPZARb"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train):\n",
        "\n",
        "  # feature_ engineering\n",
        "  train = data_recipe(train)\n",
        "\n",
        "  # Assuming you have already defined X_train and y_train\n",
        "  X_train = train\n",
        "  y_train = train['expensive']\n",
        "  del X_train['expensive']\n",
        "\n",
        "  # Standardize the features (important for some models)\n",
        "  scaler = StandardScaler()\n",
        "  X_train_standardized = scaler.fit_transform(X_train)\n",
        "\n",
        "  # Define logistic regression model\n",
        "  model = LogisticRegression1()\n",
        "\n",
        "  # Create k-fold cross-validation iterator\n",
        "  kf = KFold(n_splits=10, shuffle=True, random_state=2023)\n",
        "\n",
        "  # Lists to store cross-validation results\n",
        "  roc_auc_scores = []\n",
        "\n",
        "  # Perform k-fold cross-validation\n",
        "  for train_index, test_index in kf.split(X_train_standardized):\n",
        "      X_train_fold, X_test_fold = X_train_standardized[train_index], X_train_standardized[test_index]\n",
        "      y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "      # Fit the logistic regression model on the training fold\n",
        "      model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "      # Make predictions on the test fold\n",
        "      y_pred_fold = model.predict_proba(X_test_fold)\n",
        "\n",
        "      # Evaluate the model on the current fold\n",
        "      roc_auc_fold = roc_auc_score(y_test_fold, y_pred_fold)\n",
        "\n",
        "      roc_auc_scores.append(roc_auc_fold)\n",
        "\n",
        "  # Calculate the mean scores across all folds\n",
        "  mean_roc_auc = np.mean(roc_auc_scores)\n",
        "\n",
        "  print(f'Mean ROC AUC: {mean_roc_auc}')\n",
        "\n",
        "  return mean_roc_auc\n"
      ],
      "metadata": {
        "id": "Ebc-6Vm0bndT"
      },
      "id": "Ebc-6Vm0bndT",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_pca(train, n_components=13):\n",
        "\n",
        "    # Feature engineering\n",
        "    X_train = data_recipe(train)\n",
        "    y_train = train['expensive']\n",
        "    del X_train['expensive']\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_standardized = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA1(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_standardized)\n",
        "\n",
        "    # Define logistic regression model\n",
        "    model = LogisticRegression1()\n",
        "\n",
        "    # Create k-fold cross-validation iterator\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=2023)\n",
        "\n",
        "    # Lists to store cross-validation results\n",
        "    roc_auc_scores = []\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    for train_index, test_index in kf.split(X_train_pca):\n",
        "        X_train_fold, X_test_fold = X_train_pca[train_index], X_train_pca[test_index]\n",
        "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "        # Fit the logistic regression model on the training fold\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Make predictions on the test fold\n",
        "        y_pred_fold = model.predict_proba(X_test_fold)\n",
        "\n",
        "        # Evaluate the model on the current fold\n",
        "        roc_auc_fold = roc_auc_score(y_test_fold, y_pred_fold)\n",
        "\n",
        "        roc_auc_scores.append(roc_auc_fold)\n",
        "\n",
        "    # Calculate the mean scores across all folds\n",
        "    mean_roc_auc = np.mean(roc_auc_scores)\n",
        "\n",
        "    print(f'Mean ROC AUC with PCA: {mean_roc_auc}')\n",
        "\n",
        "    return mean_roc_auc"
      ],
      "metadata": {
        "id": "58PTUpnEYm_W"
      },
      "id": "58PTUpnEYm_W",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test model**"
      ],
      "metadata": {
        "id": "WdeowUiXZLUF"
      },
      "id": "WdeowUiXZLUF"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(train,test,test_res):\n",
        "\n",
        "  X_train = data_recipe(train)\n",
        "  test = data_recipe_test(test,train)\n",
        "  model = LogisticRegression1()\n",
        "\n",
        "  # Assuming you have already defined X_train and y_train\n",
        "  y_train = X_train['expensive']\n",
        "  del X_train['expensive']\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = model.predict_proba(test)\n",
        "\n",
        "  # Evaluate the model\n",
        "  roc_auc = roc_auc_score(test_res['expensive'], y_pred)\n",
        "\n",
        "  print(\"ROC AUC Score:\", roc_auc)\n",
        "\n",
        "  # Plot the Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC).\n",
        "  def plot_roc_auc(y_true, y_scores, title='ROC Curve'):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "  # plot_roc_auc(test_res['expensive'], y_pred, title='ROC Curve')\n",
        "\n",
        "  return roc_auc, y_pred"
      ],
      "metadata": {
        "id": "DKWF9j4ztYD5"
      },
      "id": "DKWF9j4ztYD5",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_pca(train,test,test_res):\n",
        "  # Assuming you have already defined data_recipe and data_recipe_test functions\n",
        "  X_train = data_recipe(train)\n",
        "  X_test = data_recipe_test(test, train)\n",
        "\n",
        "  # Separate the target variable\n",
        "  y_train = X_train['expensive']\n",
        "  del X_train['expensive']\n",
        "\n",
        "  # Initialize PCA with the desired number of components\n",
        "  n_components = 13  # You can adjust this based on your requirements\n",
        "  pca = PCA1(n_components=n_components)\n",
        "\n",
        "  # Fit and transform the training data\n",
        "  pca.fit(X_train)\n",
        "  X_train_pca = pca.transform(X_train)\n",
        "\n",
        "  # Transform the test data using the same PCA transformation\n",
        "  X_test_pca = pca.transform(X_test)\n",
        "\n",
        "  # Initialize Logistic Regression model\n",
        "  model = LogisticRegression1()\n",
        "\n",
        "  # Fit the model on the PCA-transformed training data\n",
        "  model.fit(X_train_pca, y_train)\n",
        "\n",
        "  # Make predictions on the test set using PCA-transformed data\n",
        "  y_pred = model.predict_proba(X_test_pca)\n",
        "\n",
        "  # Evaluate the model using ROC AUC score\n",
        "  roc_auc = roc_auc_score(test_res['expensive'], y_pred)\n",
        "\n",
        "  print(\"ROC AUC Score:\", roc_auc)\n",
        "\n",
        "  # Plot the Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC).\n",
        "  def plot_roc_auc(y_true, y_scores, title='ROC Curve'):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "  # plot_roc_auc(test_res['expensive'], y_pred, title='ROC Curve')\n",
        "\n",
        "  return roc_auc, y_pred"
      ],
      "metadata": {
        "id": "sBT0F7WYiQ5h"
      },
      "id": "sBT0F7WYiQ5h",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Testing***"
      ],
      "metadata": {
        "id": "lPJhWXAqZWw_"
      },
      "id": "lPJhWXAqZWw_"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2023)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test_res = pd.read_csv('test_lab.csv')\n",
        "\n",
        "# train_model(train)\n",
        "# train_model_pca(train)\n",
        "\n",
        "test_model(train,test,test_res)\n",
        "# test_model_pca(train,test,test_res)"
      ],
      "metadata": {
        "id": "t6dFm_wBYVcb"
      },
      "id": "t6dFm_wBYVcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code for analayze**"
      ],
      "metadata": {
        "id": "1rL3QtyvacFW"
      },
      "id": "1rL3QtyvacFW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Generates a count plot using Seaborn to visualize the distribution of properties based on any column and the 'expensive' label in a dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "x7rUoRc7bYlW"
      },
      "id": "x7rUoRc7bYlW"
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "# Count plot with seaborn based on property_type and expensive\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.countplot(x='property_type', hue='expensive', data=train, palette='viridis')\n",
        "plt.title('Distribution of Properties Based on Property Type and Expensive Label')\n",
        "plt.xlabel('Property Type')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Expensive', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Rotate x-axis labels\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FAGXXvr2iwrm"
      },
      "id": "FAGXXvr2iwrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Find the optimal number of clusters (K) using the elbow method.\n",
        "\n"
      ],
      "metadata": {
        "id": "yjwxHGugcHYV"
      },
      "id": "yjwxHGugcHYV"
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "df = train[train['expensive'] == 0]\n",
        "Coordinate = df[['latitude', 'longitude']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Coordinate_standardized = scaler.fit_transform(Coordinate)\n",
        "\n",
        "# Using the elbow method to find the optimal number of clusters\n",
        "inertia = []\n",
        "\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(Coordinate_standardized)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plotting the elbow curve\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "em-KHOPObudb"
      },
      "id": "em-KHOPObudb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Find the optimal number of clusters (K) using the Silhouette Method.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWIAuD1cdPUY"
      },
      "id": "lWIAuD1cdPUY"
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "df = train[train['expensive'] == 0]\n",
        "Coordinate = df[['latitude', 'longitude']]\n",
        "\n",
        "# Set k clusters range\n",
        "k_values = range(2, 11)\n",
        "\n",
        "# Calculate silhouette scores for different cluster numbers\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(Coordinate)\n",
        "    silhouette_avg = silhouette_score(Coordinate, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plotting the Silhouette Score curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lUYpAoW1O3Hg"
      },
      "id": "lUYpAoW1O3Hg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Creat a scatter plot to visualize the distribution of expensive properties based on latitude and longitude, along with the centroids of the clusters obtained using KMeans clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ck9pBcRFd-te"
      },
      "id": "Ck9pBcRFd-te"
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "df = train[train['expensive'] == 1]\n",
        "\n",
        "# Fit KMeans with 4 clusters for expensive properties\n",
        "kmeans_expensive = KMeans(n_clusters=4, random_state=21).fit(df[['latitude', 'longitude']])\n",
        "centroid_coordinates_expensive = kmeans_expensive.cluster_centers_\n",
        "\n",
        "# Plot existing points\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.scatterplot(x='longitude', y='latitude', hue='expensive', data=df, palette='viridis')\n",
        "plt.title('Distribution of Expensive Properties Based on Latitude and Longitude')\n",
        "plt.xlabel('Latitude')\n",
        "plt.ylabel('Longitude')\n",
        "plt.legend(title='Expensive', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Plot centroids for expensive properties\n",
        "for i, (centroid_latitude, centroid_longitude) in enumerate(centroid_coordinates_expensive):\n",
        "    plt.scatter(centroid_longitude, centroid_latitude, color='red', marker='X', s=100, label=f'Expensive Centroid {i+1}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WRGgGDGQVsp-"
      },
      "id": "WRGgGDGQVsp-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Creat a scatter plot to visualize the distribution of non-expensive properties based on latitude and longitude, along with the centroids of the clusters obtained using KMeans clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "NnvSeS0beOrp"
      },
      "id": "NnvSeS0beOrp"
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "df = train[train['expensive'] == 0]\n",
        "\n",
        "# Fit KMeans with 4 clusters for expensive properties\n",
        "kmeans_expensive = KMeans(n_clusters=4, random_state=21).fit(df[['latitude', 'longitude']])\n",
        "centroid_coordinates_expensive = kmeans_expensive.cluster_centers_\n",
        "\n",
        "# Plot existing points\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.scatterplot(x='longitude', y='latitude', hue='expensive', data=df, palette='viridis')\n",
        "plt.title('Distribution of Expensive Properties Based on Latitude and Longitude')\n",
        "plt.xlabel('Latitude')\n",
        "plt.ylabel('Longitude')\n",
        "plt.legend(title='Expensive', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Plot centroids for expensive properties\n",
        "for i, (centroid_latitude, centroid_longitude) in enumerate(centroid_coordinates_expensive):\n",
        "    plt.scatter(centroid_longitude, centroid_latitude, color='red', marker='X', s=100, label=f'Expensive Centroid {i+1}')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O36sTqYpTFnS"
      },
      "id": "O36sTqYpTFnS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The code snippet explores and prints the counts of expensive and not expensive properties, and as we can see the data is not balance.\n",
        "\n"
      ],
      "metadata": {
        "id": "cuq0dHejh1cv"
      },
      "id": "cuq0dHejh1cv"
    },
    {
      "cell_type": "code",
      "source": [
        "expensive_counts = train['expensive'].value_counts()\n",
        "print(f'Number of Expensive Properties (labeled as 1): {expensive_counts[1]}')\n",
        "print(f'Number of Not Expensive Properties (labeled as 0): {expensive_counts[0]}')\n",
        "\n",
        "# Bar plot\n",
        "plt.bar(['Expensive (1)', 'Not Expensive (0)'], expensive_counts, color=['#7EB3D1', '#FFD08A'])\n",
        "plt.title('Number of Expensive and Not Expensive Properties')\n",
        "plt.xlabel('Property Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r9utTeUbgOom"
      },
      "id": "r9utTeUbgOom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The variable 'correlations' will hold the correlation coefficients between each feature and the target variable, providing insights into the linear relationships between features and the target.\n",
        "\n"
      ],
      "metadata": {
        "id": "nv4BBmambo51"
      },
      "id": "nv4BBmambo51"
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "features = train.drop(columns=['expensive'])\n",
        "target = train['expensive']\n",
        "\n",
        "# Calculate correlations\n",
        "correlations = features.corrwith(target)\n",
        "correlations"
      ],
      "metadata": {
        "id": "EmYMCDiEbhGv"
      },
      "id": "EmYMCDiEbhGv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "swRKrrNrGye8",
        "1cVyCp9N3u9b",
        "Idsy7fec4IoL",
        "Hm5WOcSl4VQ4",
        "SMxejDMQ6TvW",
        "gtd0YMkbY57t",
        "_dw60haPZARb",
        "WdeowUiXZLUF",
        "lPJhWXAqZWw_"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}